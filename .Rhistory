alpha = 0.5,
family = gaussian)
# try elastic net for variable selection
X <- data[,!names(data) %in% c('quality')] %>% as.matrix()
y <- data$quality
en <- cv.glmnet(X,
y,
type.measure = 'mse',
alpha = 0.5,
family = gaussian)
? cv.glmnet
en <- cv.glmnet(X,y)
plot(en)
dev.off()
plot(en)
plot(en)
# set working directory
setwd('/Users/catherinesmith/Desktop/unc_bootcamp/finalProject')
# load packages
library(tidyverse)
library(MASS)
library(glmnet)
#set seed
set.seed(42)
# read in data
data <- read.csv('winequality-red.csv', sep = ';')
# full OLS model, predict quality
model <-lm(data = data, quality ~ .)
summary(model)
# compute variance inflation factors
# VIF exceeds 5 for fixed acidity anf density, indicating a problematic
# degree of multicollinearity among these predictors
vif(model)
# make diagnostic plots
# it is clear that condition of homoscedasticity among predictors
# is not met by the pattern among residuals observed in the
# Scale-Location plot, meaning that the variance of predictors is not
# independent of their values.
plot(model)
citation()
citation('tidyverse')
citation('glmnet')
en <- cv.glmnet(X,y, type.measure = 'mse', alpha=1, family='gaussian')
# set working directory
setwd('/Users/catherinesmith/Desktop/unc_bootcamp/finalProject')
# load packages
library(tidyverse)
library(MASS)
library(glmnet)
#set seed
set.seed(42)
# read in data
data <- read.csv('winequality-red.csv', sep = ';')
# long_data <- data %>%
#   pivot_longer(colnames(data)) %>%
#   as.data.frame()
#
# ggplot(long_data, aes(x=value))+
#   geom_histogram()+
#   facet_wrap(~name, scales = 'free') +
#   theme_minimal()+
#   theme(panel.grid = element_blank())
# ggsave('predictor_densities.png')
# full OLS model, predict quality
model <-lm(data = data, quality ~ .)
summary(model)
# compute variance inflation factors
# VIF exceeds 5 for fixed acidity anf density, indicating a problematic
# degree of multicollinearity among these predictors
vif(model)
# make diagnostic plots
# it is clear that condition of homoscedasticity among predictors
# is not met by the pattern among residuals observed in the
# Scale-Location plot, meaning that the variance of predictors is not
# independent of their values.
plot(model)
# apply box-cox power transformation
bc <- boxcox(model, lambda = seq(-3, 3))
best_lambda <- bc$x[which(bc$y==max(bc$y))]
# try elastic net for variable selection
# by default, 10-fold cross-validation is performed
X <- data[,!names(data) %in% c('quality')] %>% as.matrix()
y <- data$quality
en <- cv.glmnet(X,y, type.measure = 'mse', alpha=1, family='gaussian')
plot(en)
train_rows <- sample(1:nrow(X), 0.7*nrow(X))
X.train <- X[train_rows,]
X.test <- X[-train_rows,]
y.train <- y[train_rows,]
# set working directory
setwd('/Users/catherinesmith/Desktop/unc_bootcamp/finalProject')
# load packages
library(tidyverse)
library(MASS)
library(glmnet)
#set seed
set.seed(42)
# read in data
data <- read.csv('winequality-red.csv', sep = ';')
# long_data <- data %>%
#   pivot_longer(colnames(data)) %>%
#   as.data.frame()
#
# ggplot(long_data, aes(x=value))+
#   geom_histogram()+
#   facet_wrap(~name, scales = 'free') +
#   theme_minimal()+
#   theme(panel.grid = element_blank())
# ggsave('predictor_densities.png')
# full OLS model, predict quality
model <-lm(data = data, quality ~ .)
summary(model)
# compute variance inflation factors
# VIF exceeds 5 for fixed acidity anf density, indicating a problematic
# degree of multicollinearity among these predictors
vif(model)
# make diagnostic plots
# it is clear that condition of homoscedasticity among predictors
# is not met by the pattern among residuals observed in the
# Scale-Location plot, meaning that the variance of predictors is not
# independent of their values.
plot(model)
# apply box-cox power transformation
bc <- boxcox(model, lambda = seq(-3, 3))
best_lambda <- bc$x[which(bc$y==max(bc$y))]
# try elastic net for variable selection
# by default, 10-fold cross-validation is performed
X <- data[,!names(data) %in% c('quality')] %>% as.matrix()
y <- data$quality
# split into train and test
train_rows <- sample(1:nrow(X), 0.7*nrow(X))
X.train <- X[train_rows,]
X.test <- X[-train_rows,]
y.train <- y[train_rows,]
y.train <- y[train_rows]
y.test < -y[-train_rows]
y.test <- y[-train_rows]
en.predicted <-predict(en, s=en$lmbda.1se, newx=X.test)
en.predicted <-predict(en, s=en$lambda.1se, newx=X.test)
# set working directory
setwd('/Users/catherinesmith/Desktop/unc_bootcamp/finalProject')
# load packages
library(tidyverse)
library(MASS)
library(glmnet)
#set seed
set.seed(42)
# read in data
data <- read.csv('winequality-red.csv', sep = ';')
# generate pair plots to visualize relationship among predictors
long_data <- data %>%
pivot_longer(colnames(data)) %>%
as.data.frame()
ggplot(long_data, aes(x=value))+
geom_histogram()+
facet_wrap(~name, scales = 'free') +
theme_minimal()+
theme(panel.grid = element_blank())
ggsave('predictor_densities.png')
# full OLS model, predict quality
model <-lm(data = data, quality ~ .)
summary(model)
# compute variance inflation factors
# VIF exceeds 5 for fixed acidity anf density, indicating a problematic
# degree of multicollinearity among these predictors
vif(model)
# make diagnostic plots
# it is clear that condition of homoscedasticity among predictors
# is not met by the pattern among residuals observed in the
# Scale-Location plot, meaning that the variance of predictors is not
# independent of their values.
jpeg(file = "linearDiganosticPlots.jpg")
par(mfrow=c(2,2))
plot(model)
dev.off()
# try transforming the data to deal with lack of multivariate normality
# apply box-cox power transformation
bc <- boxcox(model, lambda = seq(-3, 3))
best_lambda <- bc$x[which(bc$y==max(bc$y))]
X <- data[,!names(data) %in% c('quality')] %>% as.matrix()
y <- data$quality
# split into train and test
train_rows <- sample(1:nrow(X), 0.7*nrow(X))
X.train <- X[train_rows,]
X.test <- X[-train_rows,]
y.train <- y[train_rows]
y.test <- y[-train_rows]
# try elastic net for variable selection
# by default, 10-fold cross-validation is performed
lasso <- cv.glmnet(X.train, y.train, type.measure = 'mse', alpha=1, family='gaussian')
lasso.predicted <-predict(lasso, s=lasso$lambda.1se, newx=X.test)
mean((y.test - lasso.predicted)^2)
ridge <- cv.glmnet(X.train, y.train, type.measure = 'mse', alpha=0, family='gaussian')
ridge.predicted <-predict(ridge, s=ridge$lambda.1se, newx=X.test)
mean((y.test - ridge.predicted)^2)
# try elastic net, optimize alpha
alpha_list<- list()
mean_errors <- list()
for(i in 0:10){
fit.name <- paste0('alpha', i/10)
alpha_list[[fit.name]] <- cv.glmnet(X.train, y.train, type.measure = 'mse', alpha=i/10, family='gaussian')
mean_list <- mean((y.test - alpha_list[[fit.name]])^2)
}
# try elastic net, optimize alpha
alpha_list<- list()
mean_errors <- list()
for(i in 0:10){
fit.name <- paste0('alpha', i/10)
alpha_list[[fit.name]] <- cv.glmnet(X.train, y.train, type.measure = 'mse', alpha=i/10, family='gaussian')
mean_list <- mean((y.test - alpha_list[[fit.name]].predicted)^2)
# try elastic net, optimize alpha
alpha_list<- list()
mean_errors <- list()
for(i in 0:10){
fit.name <- paste0('alpha', i/10)
alpha_list[[fit.name]] <- cv.glmnet(X.train, y.train, type.measure = 'mse', alpha=i/10, family='gaussian')
}
View(alpha_list)
results <-data.frame()
# set working directory
setwd('/Users/catherinesmith/Desktop/unc_bootcamp/finalProject')
# load packages
library(tidyverse)
library(MASS)
library(glmnet)
#set seed
set.seed(42)
# read in data
data <- read.csv('winequality-red.csv', sep = ';')
# generate pair plots to visualize relationship among predictors
long_data <- data %>%
pivot_longer(colnames(data)) %>%
as.data.frame()
ggplot(long_data, aes(x=value))+
geom_histogram()+
facet_wrap(~name, scales = 'free') +
theme_minimal()+
theme(panel.grid = element_blank())
ggsave('predictor_densities.png')
# full OLS model, predict quality
model <-lm(data = data, quality ~ .)
summary(model)
# compute variance inflation factors
# VIF exceeds 5 for fixed acidity anf density, indicating a problematic
# degree of multicollinearity among these predictors
vif(model)
# make diagnostic plots
# it is clear that condition of homoscedasticity among predictors
# is not met by the pattern among residuals observed in the
# Scale-Location plot, meaning that the variance of predictors is not
# independent of their values.
jpeg(file = "linearDiganosticPlots.jpg")
par(mfrow=c(2,2))
plot(model)
dev.off()
# try transforming the data to deal with lack of multivariate normality
# apply box-cox power transformation
bc <- boxcox(model, lambda = seq(-3, 3))
best_lambda <- bc$x[which(bc$y==max(bc$y))]
X <- data[,!names(data) %in% c('quality')] %>% as.matrix()
y <- data$quality
# split into train and test
train_rows <- sample(1:nrow(X), 0.7*nrow(X))
X.train <- X[train_rows,]
X.test <- X[-train_rows,]
y.train <- y[train_rows]
y.test <- y[-train_rows]
# try lasso for variable selection
# by default, 10-fold cross-validation is performed
lasso <- cv.glmnet(X.train, y.train, type.measure = 'mse', alpha=1, family='gaussian')
lasso.predicted <-predict(lasso, s=lasso$lambda.1se, newx=X.test)
mean((y.test - lasso.predicted)^2)
# try ridge regression
ridge <- cv.glmnet(X.train, y.train, type.measure = 'mse', alpha=0, family='gaussian')
ridge.predicted <-predict(ridge, s=ridge$lambda.1se, newx=X.test)
mean((y.test - ridge.predicted)^2)
# try elastic net, optimize alpha
alpha_list<- list()
mean_errors <- list()
results <-data.frame()
for(i in 0:10){
fit.name <- paste0('alpha', i/10)
alpha_list[[fit.name]] <- cv.glmnet(X.train, y.train, type.measure = 'mse', alpha=i/10, family='gaussian')
predicted <- predict(alpha_list[[fit.name]], s=alpha_list[[fit.name]]$lambda.1se, newx=X.test)
mse <- mean((y.test-predicted)^2)
temp < - data.frame(alpha=i/10, mse=mse, fit.name=fit.name)
results = rbind(results, temp)
}
# set working directory
setwd('/Users/catherinesmith/Desktop/unc_bootcamp/finalProject')
# load packages
library(tidyverse)
library(MASS)
library(glmnet)
#set seed
set.seed(42)
# read in data
data <- read.csv('winequality-red.csv', sep = ';')
# generate pair plots to visualize relationship among predictors
long_data <- data %>%
pivot_longer(colnames(data)) %>%
as.data.frame()
ggplot(long_data, aes(x=value))+
geom_histogram()+
facet_wrap(~name, scales = 'free') +
theme_minimal()+
theme(panel.grid = element_blank())
ggsave('predictor_densities.png')
# full OLS model, predict quality
model <-lm(data = data, quality ~ .)
summary(model)
# compute variance inflation factors
# VIF exceeds 5 for fixed acidity anf density, indicating a problematic
# degree of multicollinearity among these predictors
vif(model)
# make diagnostic plots
# it is clear that condition of homoscedasticity among predictors
# is not met by the pattern among residuals observed in the
# Scale-Location plot, meaning that the variance of predictors is not
# independent of their values.
jpeg(file = "linearDiganosticPlots.jpg")
par(mfrow=c(2,2))
plot(model)
dev.off()
# try transforming the data to deal with lack of multivariate normality
# apply box-cox power transformation
bc <- boxcox(model, lambda = seq(-3, 3))
best_lambda <- bc$x[which(bc$y==max(bc$y))]
X <- data[,!names(data) %in% c('quality')] %>% as.matrix()
y <- data$quality
# split into train and test
train_rows <- sample(1:nrow(X), 0.7*nrow(X))
X.train <- X[train_rows,]
X.test <- X[-train_rows,]
y.train <- y[train_rows]
y.test <- y[-train_rows]
# try lasso for variable selection
# by default, 10-fold cross-validation is performed
lasso <- cv.glmnet(X.train, y.train, type.measure = 'mse', alpha=1, family='gaussian')
lasso.predicted <-predict(lasso, s=lasso$lambda.1se, newx=X.test)
mean((y.test - lasso.predicted)^2)
# try ridge regression
ridge <- cv.glmnet(X.train, y.train, type.measure = 'mse', alpha=0, family='gaussian')
ridge.predicted <-predict(ridge, s=ridge$lambda.1se, newx=X.test)
mean((y.test - ridge.predicted)^2)
# try elastic net, optimize alpha
alpha_list<- list()
mean_errors <- list()
results <-data.frame()
for(i in 0:10){
fit.name <- paste0('alpha', i/10)
alpha_list[[fit.name]] <- cv.glmnet(X.train, y.train, type.measure = 'mse', alpha=i/10, family='gaussian')
predicted <- predict(alpha_list[[fit.name]], s=alpha_list[[fit.name]]$lambda.1se, newx=X.test)
mse <- mean((y.test-predicted)^2)
temp <- data.frame(alpha=i/10, mse=mse, fit.name=fit.name)
results = rbind(results, temp)
}
View(results)
# set working directory
setwd('/Users/catherinesmith/Desktop/unc_bootcamp/finalProject')
# load packages
library(tidyverse)
library(MASS)
library(glmnet)
# read in data
data <- read.csv('winequality-red.csv', sep = ';')
# generate pair plots to visualize relationship among predictors
long_data <- data %>%
pivot_longer(colnames(data)) %>%
as.data.frame()
ggplot(long_data, aes(x=value))+
geom_histogram()+
facet_wrap(~name, scales = 'free') +
theme_minimal()+
theme(panel.grid = element_blank())
ggsave('predictor_densities.png')
# full OLS model, predict quality
model <-lm(data = data, quality ~ .)
summary(model)
# compute variance inflation factors
# VIF exceeds 5 for fixed acidity anf density, indicating a problematic
# degree of multicollinearity among these predictors
vif(model)
# make diagnostic plots
# it is clear that condition of homoscedasticity among predictors
# is not met by the pattern among residuals observed in the
# Scale-Location plot, meaning that the variance of predictors is not
# independent of their values.
jpeg(file = "linearDiganosticPlots.jpg")
par(mfrow=c(2,2))
plot(model)
dev.off()
# try transforming the data to deal with lack of multivariate normality
# apply box-cox power transformation
bc <- boxcox(model, lambda = seq(-3, 3))
best_lambda <- bc$x[which(bc$y==max(bc$y))]
X <- data[,!names(data) %in% c('quality')] %>% as.matrix()
y <- data$quality
# split into train and test
train_rows <- sample(1:nrow(X), 0.7*nrow(X))
X.train <- X[train_rows,]
X.test <- X[-train_rows,]
y.train <- y[train_rows]
y.test <- y[-train_rows]
# try lasso for variable selection
# by default, 10-fold cross-validation is performed
lasso <- cv.glmnet(X.train, y.train, type.measure = 'mse', alpha=1, family='gaussian')
lasso.predicted <-predict(lasso, s=lasso$lambda.1se, newx=X.test)
mean((y.test - lasso.predicted)^2)
# try ridge regression
ridge <- cv.glmnet(X.train, y.train, type.measure = 'mse', alpha=0, family='gaussian')
ridge.predicted <-predict(ridge, s=ridge$lambda.1se, newx=X.test)
mean((y.test - ridge.predicted)^2)
# try elastic net, optimize alpha
alpha_list<- list()
mean_errors <- list()
results <-data.frame()
for(i in 0:10){
fit.name <- paste0('alpha', i/10)
alpha_list[[fit.name]] <- cv.glmnet(X.train, y.train, type.measure = 'mse', alpha=i/10, family='gaussian')
predicted <- predict(alpha_list[[fit.name]], s=alpha_list[[fit.name]]$lambda.1se, newx=X.test)
mse <- mean((y.test-predicted)^2)
temp <- data.frame(alpha=i/10, mse=mse, fit.name=fit.name)
results = rbind(results, temp)
}
View(alpha_list)
View(results)
# set working directory
setwd('/Users/catherinesmith/Desktop/unc_bootcamp/finalProject')
# load packages
library(tidyverse)
library(MASS)
library(glmnet)
# read in data
data <- read.csv('winequality-red.csv', sep = ';')
# generate pair plots to visualize relationship among predictors
long_data <- data %>%
pivot_longer(colnames(data)) %>%
as.data.frame()
ggplot(long_data, aes(x=value))+
geom_histogram()+
facet_wrap(~name, scales = 'free') +
theme_minimal()+
theme(panel.grid = element_blank())
ggsave('predictor_densities.png')
# full OLS model, predict quality
model <-lm(data = data, quality ~ .)
summary(model)
# compute variance inflation factors
# VIF exceeds 5 for fixed acidity anf density, indicating a problematic
# degree of multicollinearity among these predictors
vif(model)
# make diagnostic plots
# it is clear that condition of homoscedasticity among predictors
# is not met by the pattern among residuals observed in the
# Scale-Location plot, meaning that the variance of predictors is not
# independent of their values.
jpeg(file = "linearDiganosticPlots.jpg")
par(mfrow=c(2,2))
plot(model)
dev.off()
# try transforming the data to deal with lack of multivariate normality
# apply box-cox power transformation
bc <- boxcox(model, lambda = seq(-3, 3))
best_lambda <- bc$x[which(bc$y==max(bc$y))]
X <- data[,!names(data) %in% c('quality')] %>% as.matrix()
y <- data$quality
# split into train and test
train_rows <- sample(1:nrow(X), 0.7*nrow(X))
X.train <- X[train_rows,]
X.test <- X[-train_rows,]
y.train <- y[train_rows]
y.test <- y[-train_rows]
# try lasso for variable selection
# by default, 10-fold cross-validation is performed
lasso <- cv.glmnet(X.train, y.train, type.measure = 'mse', alpha=1, family='gaussian')
lasso.predicted <-predict(lasso, s=lasso$lambda.1se, newx=X.test)
mean((y.test - lasso.predicted)^2)
# try ridge regression
ridge <- cv.glmnet(X.train, y.train, type.measure = 'mse', alpha=0, family='gaussian')
ridge.predicted <-predict(ridge, s=ridge$lambda.1se, newx=X.test)
mean((y.test - ridge.predicted)^2)
# try elastic net, optimize alpha
alpha_list<- list()
mean_errors <- list()
results <-data.frame()
for(i in 0:10){
fit.name <- paste0('alpha', i/10)
alpha_list[[fit.name]] <- cv.glmnet(X.train, y.train, type.measure = 'mse', alpha=i/10, family='gaussian')
predicted <- predict(alpha_list[[fit.name]], s=alpha_list[[fit.name]]$lambda.1se, newx=X.test)
mse <- mean((y.test-predicted)^2)
temp <- data.frame(alpha=i/10, mse=mse, fit.name=fit.name)
results = rbind(results, temp)
}
View(results)
# set working directory
setwd('/Users/catherinesmith/Desktop/unc_bootcamp/finalProject')
# load packages
library(tidyverse)
library(MASS)
library(glmnet) # RIDGE regression / LASSO regression / Elastic Net regression
# read in data
data <- read.csv('winequality-red.csv', sep = ';')
# generate pair plots to visualize relationship among predictors
long_data <- data %>%
pivot_longer(colnames(data)) %>%
as.data.frame()
ggplot(long_data, aes(x=value))+
geom_histogram()+
facet_wrap(~name, scales = 'free') +
theme_minimal()+
theme(panel.grid = element_blank())
